{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando Regressão Múltipla do Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    res = Y - np.dot(X,w_current)\n",
    "    b_gradient = np.sum(res)\n",
    "    X1 = X[:,1][:,np.newaxis]\n",
    "    X2 = X[:,2][:,np.newaxis]\n",
    "    X3 = X[:,3][:,np.newaxis]\n",
    "    X4 = X[:,4][:,np.newaxis]\n",
    "    X5 = X[:,5][:,np.newaxis]\n",
    "    m1_gradient = np.sum(np.multiply(res,X1))\n",
    "    m2_gradient = np.sum(np.multiply(res,X2))\n",
    "    m3_gradient = np.sum(np.multiply(res,X3))\n",
    "    m4_gradient = np.sum(np.multiply(res,X4))\n",
    "    m5_gradient = np.sum(np.multiply(res,X5))\n",
    "    new_w = np.array([(w_current[0] + (2 * learningRate * b_gradient)), \n",
    "             (w_current[1] + (2 * learningRate * m1_gradient)), \n",
    "             (w_current[2] + (2 * learningRate * m2_gradient)),\n",
    "             (w_current[3] + (2 * learningRate * m3_gradient)),\n",
    "             (w_current[4] + (2 * learningRate * m4_gradient)),\n",
    "             (w_current[5] + (2 * learningRate * m5_gradient)),])\n",
    "    return [new_w,b_gradient,m1_gradient,m2_gradient,m3_gradient,m4_gradient,m5_gradient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, num_iterations):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf,np.inf,np.inf,np.inf,np.inf])\n",
    "    i = 0\n",
    "    while (i < num_iterations):\n",
    "        w,b_gradient,m1_gradient,m2_gradient,m3_gradient,m4_gradient,m5_gradient = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        grad = np.array([b_gradient,m1_gradient,m2_gradient,m3_gradient,m4_gradient,m5_gradient])\n",
    "        if i % 10000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at w0 = [0.], w1 = [0.], w2 = [0.], w3 = [0.], w4 = [0.], w4 = [0.], error = [[54.47995386]]\n",
      "Running...\n",
      "MSE na iteração 0 é de [[15.39415211]]\n",
      "MSE na iteração 10000 é de [[0.42109776]]\n",
      "MSE na iteração 20000 é de [[0.41601668]]\n",
      "Gradiente descendente convergiu com w0 = [1.1715187], w1 = [0.11318679], w2 = [0.07234762], w3 = [0.1628083], w4 = [0.40992914], w4 = [0.02724569], error = [[0.41358094]]\n",
      "Versão vetorizada rodou em: 3940.9332275390625 ms\n"
     ]
    }
   ],
   "source": [
    "points = np.genfromtxt(\"sample_treino.csv\", delimiter=\",\")\n",
    "points = points[1:len(points),:]\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "X = points[:,0:6]\n",
    "Y = points[:,6][:,np.newaxis]\n",
    "init_w = np.zeros((6,1))\n",
    "learning_rate = 0.00003\n",
    "num_iterations = 30000\n",
    "print(\"Starting gradient descent at w0 = {0}, w1 = {1}, w2 = {2}, w3 = {3}, w4 = {4}, w4 = {5}, error = {6}\".format(init_w[0], init_w[1], init_w[2], init_w[3], init_w[4], init_w[5], compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, num_iterations)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, w2 = {2}, w3 = {3}, w4 = {4}, w4 = {5}, error = {6}\".format(w[0], w[1], w[2], w[3], w[4], w[5], compute_mse_vectorized(w,X,Y)))\n",
    "print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação com os coeficientes encontrados por scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1715187   0.01014536  0.02591092 -0.00129005  0.02875071  0.00696753]]\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X,Y)\n",
    "print(w.T-lm.coef_) #diferença entre os coeficientes calculados pela implementação e o scikit lear\n",
    "#Como pode ser observado, os valores foram próximos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
